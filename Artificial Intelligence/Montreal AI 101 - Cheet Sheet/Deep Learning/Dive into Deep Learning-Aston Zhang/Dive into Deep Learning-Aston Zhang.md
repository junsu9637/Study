# Dive into Deep Learning-Aston Zhang
딥러닝에 빠져들다

# 개요

1. 소개

1.1 동기 부여 예제       
1.2 주요 구성 요소              
1.3 기계 학습 문제의 종류            
1.4 뿌리             
1.5 딥러닝으로 가는 길            
1.6 성공담            
1.7 특성               

2. 준비 사항

2.1 데이터 처리        
2.1.1 시작               
2.1.2 운영             
2.1.3 통신 장치               
2.1.4 인덱싱과 슬라이싱                
2.1.5 다른 Python 개체로의 변환             

2.2 데이터 전처리             
2.2.1 데이터 세트 읽기              
2.2.2 누락된 데이터 처리             
2.2.3 Tansor 형식으로 변환             

2.3 선형 대수학              
2.3.1 스칼라             
2.3.2 벡터             
2.3.3 행렬                
2.3.4 텐서         
2.3.5 텐서 연산의 기본 특성            
2.3.6 감량           
2.3.7 도트 제품           
2.3.8 Matrix-Vector 제품            
2.3.9 Matrix-Matrix 곱셈               
2.3.10 규범           
2.3.11 추가적인 선형 대수학              

2.4 미적분학              
2.4.1 파생 모델과 차별화            
2.4.2 부분 파생 모델           
2.4.3 경사도            
2.4.4 체인 규칙             

2.5 자동 차별화           
2.5.1 간단한 예시             
2.5.2 비스칼라 변수의 후진           
2.5.3 계산의 분리             
2.5.4 Python의 제어 흐름 경사도 계산             

2.6 확률         
2.6.1 기본 확률론            
2.6.2 다중 랜덤 변수 처리           
2.6.3 기대값 및 분산                

2.7 문서              
2.7.1 모듈의 모든 함수와 클래스 찾기               
2.7.2 특정 함수와 클래스 사용법 찾기            

3. 선형 신경망                

3.1 선형 회귀 분석           
3.1.1 선형 회귀 분석의 기본 요소             
3.1.2 속도에 대한 벡터화              
3.1.3 정규 분포 및 제곱 손실             
3.1.4 선형 회귀 분석 기반 심층 네트워크               

3.2 Scratch의 선형 회귀 분석 구현         
3.2.1 데이터 세트 생성             
3.2.2 데이터 세트 읽기              
3.2.3 모델 매개 변수 초기화              
3.2.4 모델 정의              
3.2.5 손실 함수 정의              
3.2.6 최적화 알고리즘 정의             
3.2.7 학습            

3.3 선형 회귀 분석의 간결한 구현           
3.3.1 데이터 세트 생성               
3.3.2 데이터 세트 읽기            
3.3.3 모델 매개 변수 초기화            
3.3.4 모델 정의              
3.3.5 손실 함수 정의             
3.3.6 최적화 알고리즘 정의             
3.3.7 학습               

3.4 소프트맥스 회귀 분석             
3.4.1 분류 문제              
3.4.2 네트워크 아키텍쳐             
3.4.3 소프트멕스 운영              
3.4.4 미니배치에 대한 벡터화             
3.4.5 손실 함수                
3.4.6 기본 정보 이론         
3.4.7 모델 예측 및 평가                

3.5 이미지 분류 데이터 세트                 
3.5.1 데이터 세트 읽기             
3.5.2 미니배치 읽기             
3.5.3 모든 정보 종합               

3.6 Scratch의 소프트멕스 회귀 분석 구현               
3.6.1 모델 매개 변수 초기화             
3.6.2 소프트멕스 운영 정의              
3.6.3 모델 정의              
3.6.4 손실 함수 정의             
3.6.5 분류 정확도             
3.6.6 학습            
3.6.7 예측            

3.7 소프트멕스 회귀 분석의 간결한 구현         
3.7.1 모델 매개 변수 초기화          
3.7.2 소프트멕스 구현             
3.7.3 소프트멕스 최적화 알고리즘               
3.7.4 학습              

4. 다층 퍼셉트론              

4.1 다층 퍼셉트론              
4.1.1 은닉 계층            
4.1.2 활성화 함수            

4.2 Scratch의 다층 퍼셉트론 구현        
4.2.1 모델 매개 변수 초기화           
4.2.2 활성화 함수            
2.2.3 모델           
2.2.4 손실 함수            
2.2.5 학습              

4.3 다층 퍼셉트론의 간결한 구현             
4.3.1 모델              

4.4 모델 선정과 과소 적합, 과대 적합                
4.4.1 학습 오류 및 일반화 오류                  
4.4.2 모델 선정         
4.4.3 과소 적합 또는 과대 적합               
4.4.4 다항식 회귀 분석              

4.5 가중치 감소          
4.5.1 규범과 가중치 감소                
4.5.2 고차원 선형 회귀 분석           
4.5.3 Scratch의 구현              
4.5.4 간략한 구현             

4.6 드롭아웃            
4.6.1 과적합의 시작             
4.6.2 작은 변화를 통한 강건함            
4.6.3 학습에서 드롭아웃            
4.6.4 Scratch의 구현                 
4.6.5 간결한 구현               

4.7 정전파, 역전파, 계산 그래프               
4.7.1 정전파              
4.7.2 정전파의 계산 그래프          
4.7.3 역잔파             
4.7.4 신경망 훈련             

4.8 수치 안정성 및 초기화             
4.8.1 경사도의 소멸 및 폭발              
4.8.2 매개 변수 초기화               

4.9 환경 및 분포 변화               
4.9.1 분포 변화 유형             
4.9.2 분포 변화 예시            
4.9.3 분포 변화 수정              
4.9.4 학습 문제의 분류법             
4.9.5 기계 학습의 공정성, 책임성, 투명성               
                 
4.10 Kaggle의 집값 예측              
4.10.1 데이터 세트 다운로드 및 캐싱             
4.10.2 Kaggle            
4.10.3 데이터 세트 접속 및 읽기             
4.10.4 데이터 사전 처리              
4.10.5 학습             
4.10.6 K-Fold 교차 검증             
4.10.7 모델 선정              
4.10.8 Kaggle에 대한 예측               

5. 딥러닝 연산             

5.1 계층과 블록           
5.1.1 사용자 지정 블록             
5.1.2 배열 블록             
5.1.3 정전파 함수 코드 실행             
5.1.4 효율성             

5.2 매개 변수 관리             
5.2.1 매개 변수 접근             
5.2.2 매개 변수 초기화             
5.2.3 매개 변수 연결                  

5.3 초기화 지연             
5.3.1 네트워크 인스턴스화               

5.4 사용자 지정 계층              
5.4.1 매개 변수 없는 계층              
5.4.2 매개 변수 기반 계층            

5.5 I/O 파일        
5.5.1 텐서 로딩 및 저장            
5.5.2 모델 매개 변수 로딩 및 저장                  

5.6 GPU                       
5.6.1 컴퓨터 장치            
5.6.2 텐서와 GPU           
5.6.3 신경망과 GPU              

6. 컨볼루션 신경망                

6.1 완전 연결된 계층에서 컨볼루션            
6.1.1 불변성            
6.1.2 MLP 제한            
6.1.3 컨볼루션             
6.1.4 "Waldo가 어디있는가"의 재논의           

6.2 이미지에서 컨볼루션           
6.2.1 교차 상관관계 운영           
6.2.2 컨볼루션 계층           
6.2.3 이미지의 개체 엣지 탐지           
6.2.4 커널 학습           
6.2.5 교차 상관관계와 컨볼루션         
6.2.6 특징 맵과 수신 필드               

6.3 패딩과 스트라이드            
6.3.1 패딩           
6.3.2 스트라이드                 

6.4 다중 입력 및 다중 출력 채널            
6.4.1 다중 입력 채널         
6.4.2 다중 출력 채널           
6.4.3 1X1 컨볼루션 계층            

6.5 풀링          
6.5.1 최대 풀링 및 평균 풀링           
6.5.2 패팅 및 스트라이드         
6.5.3 다중 채널            

6.6 컨볼루션 신경망(LeNet)           
6.6.1 LeNet           
6.6.2 학습             

7. 현대 컨볼루션 신경망           

7.1 심층 컨볼루션 신경망(AlexNet)              
7.1.1 학습 표현         
7.1.2 AlexNet           
7.1.3 데이터 세트 읽기            
7.1.4 학습            

7.2 네트워크의 블록 사용(VGG)           
7.2.1 VGG 블록              
7.2.2 VGG 네트워크           
7.2.3 학습            

7.3 네트워크 내부 내트워크(NiN)            
7.3.1 NiN 블록          
7.3.2 NiN 모델           
7.3.3 학습              

7.4 병렬 연결 네트워크(GoogLeNet)          
7.4.1 시작 블록           
7.4.2 GoogLeNet 모델            
7.4.3 학습           

7.5 배치 정규화         
7.5.1 심층 네트워크 학습           
7.5.2 배치 정규화 계층           
7.5.3 Scratch의 구현            
7.5.4 LeNet의 배치 정규화 적용            
7.5.5 간략한 구현           
7.5.6 논란              

7.6 잔여 네트워크(ResNet)           
7.6.1 함수 클래스           
7.6.2 잔여 블록           
7.6.3 ResNet 모델         
7.6.4 학습                 

7.7 조밀하게 연결된 네트워크(DenseNet)          
7.7.1 ResNet 기반 DenseNet          
7.7.2 고밀도 블록          
7.7.3 전환 계층           
7.7.4 DenseNet 모델           
7.7.5 학습           

8. 반복 신경망           

8.1 배열모델          
8.1.1 통계 도구            
8.1.2 학습          
8.1.3 예측             

8.2 텍스트 사전 처리          
8.2.1 데이터 세트 읽기             
8.2.2 형식화          
8.2.3 어휘            
8.2.4 모든 상황 종합            

8.3 언어 모델 및 데이터 세트           
8.3.1 언어 모델 학습            
8.3.2 Markov 모델과 n-grams           
8.3.3 자연어 통계            
8.3.4 긴 배열 데이터 읽기               

8.4 순환 신경망         
8.4.1 은닉 상태 없는 신경망           
8.4.2 윽닉 상태 포함 순환 신경망            
8.4.3 RNN 기반 문자 수준 언어 모델           
8.4.4 논란            

8.5 Scratch의 순환 신경망 구현          
8.5.1 One-Hot 인코딩           
8.5.2 모델 매개 변수 초기화          
8.5.3 RNN 모델           
8.5.4 예측          
8.5.5 경사도 고정           
8.5.6 학습           

8.6 반복 신경망의 간결한 구현          
8.6.1 모델 정의         
8.6.2 학습 및 예측                       

8.7 시간 경과에 따른 역전파                 
8.7.1 RNN의 경사도 분석                     
8.7.2 자세한 시간에 따른 역전파                

9 현대 반복 신경망               

9.1 데이트 반복 유닛(GRU)              
9.1.1 게이트 은닉 상태               
9.1.2 Scratch의 구현                
9.1.3 간결한 구현                              

9.2 장, 단기 메모리(LSTM)              
9.2.1 게이트 메모리 셀             
9.2.2 Scratch의 구현             
9.2.3 간결한 구현             

9.3 심층 반복 신경망            
9.3.1 함수 종속성            
9.3.2 간략한 구현             
9.3.3 학습 및 예측             

9.4 양방향 반복 신경망             
9.4.1 숨겨진 Markov 모델의 동적 프로그래밍             
9.4.2 양방향 모델               
9.4.3 잘못된 응용 프로그램에 대한 양방향 RNN 학습                 

9.5 시스템 변환과 데이터 집합             
9.5.1 데이터 세트 다운로드 및 사전 처리              
9.5.2 형식화              
9.5.3 어휘              
9.5.4 데이터 세트 로딩             
9.5.5 모든 내용 종합                        
       
9.6 인코더-디코더 아키텍쳐             
9.6.1 인코더            
9.6.2 디코더              
9.6.3 모든 내용 종합               

9.7 배열 중 배열을 통한 학습              
9.7.1 인코더              
9.7.2 디코더             
9.7.3 손실 함수             
9.7.4 학습              
9.7.5 예측          
9.7.6 예측 배열의 평가    

9.8 기둥 검색              
9.8.1 탐욕스러운 검색          
9.8.2 전체 검색           
9.8.3 기둥 검색               

10 행동 메커니즘                   

10.1 행동 메커니즘                
10.1.1 도트 제품 행동              
10.1.2 MLP 행동                 

10.2 배열 중 배열과 행동 메커니즘              
10.2.1 디코더             
10.2.2 훈련           

10.3 변압기          
10.3.1 다중 전진 행동              
10.3.2 위치별 Feed-Forward 네크워크              
10.3.3 추가와 규범         
10.3.4 위치 인코딩              
10.3.5 인코더          
10.3.6 디코더              
10.3.7 훈련           

11. 최적화 알고리즘              

11.1 최적화와 딥러닝            
11.1.1 최적화와 추정            
11.1.2 딥러닝의 최적화 과제           

11.2 볼록함            
11.2.1 기본              
11.2.2 속성            
11.2.3 제약 조건             

11.3 경사 하강법             
11.3.1 1차원 경사 하강법            
11.3.2 다변량 경사 하강법             
11.3.3 적응형 방법            

11.4 확률적 경사 하강법               
11.4.1 확률적 경사 상승법         
11.4.2 동적 학습 속도           
11.4.3 볼록한 목표에 대한 수렴 분석             
11.4.4 확률적 경사도와 유한 표본            

11.5 미니배치 확률적 경사 하강법             
11.5.1 벡터화 및 캐시             
11.5.2 미니배치             
11.5.3 데이터 세트 읽기           
11.5.4 Scratchd의 구현             
11.5.5 간결한 구현            

11.6 가속도            
11.6.1 기본             
11.6.2 실제 시험            
11.6.3 이론적 분석             

11.7 Adagrad             
11.7.1 희소 특징 및 학습 속도           
11.7.2 전제 조건화            
11.7.3 알고리즘            
11.7.4 Scratchd의 구현            
11.7.5 간결한 구현               

11.8 RMSProp               
11.8.1 알고리즘             
11.8.2 Scratchd의 구현           
11.8.3 간결한 구현              

11.9 Adadelta            
11.9.1 알고리즘               
11.9.2 구현            

11.10 Adam            
11.10.1 알고리즘              
11.10.2 구현           
11.10.3 Yogi             

11.11 학습 속도 스케줄링             
11.11.1 모형 문제            
11.11.2 스케줄러              
11.11.3 정책            

12 연산         


