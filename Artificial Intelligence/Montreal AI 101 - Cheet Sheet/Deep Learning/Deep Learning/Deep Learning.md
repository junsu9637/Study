# Deep Learning - Yann LeCun

# 개요

Deep Learning은 여러 처리 계층으로 구성된 계산 모델을 통해 여러 수준의 추상화를 가진 데이터의 표현을 학습할 수 있다. 이러한 방법은 음성 인식, 시각적 물체 인식, 물체 감지, 약물 발견, 유전체학 같은 많은 다른 영역에서 최첨단 기술을 크게 향상시켰다. Deep Learning은 역전파 알고리즘을 사용하여 기계가 이전 계층의 표현에서 각 계층의 표현을 계산하는 데 사용되는 내부 매개 변수를 어떻게 변경해야 하는지를 표시함으로써 대규모 데이터 세트의 복잡한 구조를 발견한다. 심층 컨볼루션 망은 이미지, 비디오, 음성 및 오디오 처리에 획기적인 발전을 가져온 반면, 반복 망은 텍스트와 음성 같은 순차적 데이터에 발전을 가져왔다.

## 목차

[**1. 서론**](#서론)     
[**2. Supervised learning(지도학습)**](#supervised-learning)         
[**3. 여러 계층 아키텍처 교육을 위한 Backpropagation(역전파)**](#여러-계층-아키텍처-교육을-위한-backpropagation)      
[**4. Convolutional neural networks(컨볼루션 신경망)**](#convolutional-neural-networks)         
[**5. Deep Convolutional neural networks를 통한 이미지 이해**](#deep-convolutional-neural-networks를-통한-이미지-이해)           
[**6. 분산 표현 및 언어 처리**](#분산-표현-및-언어-처리)           
[**7. 반복 신경망**](#반복-신경망)                   
[**8. Deep Learning의 미래**](#deep-learning의-미래)            

# 서론

Machine Learning 기술은 현대 사회의 많은 측면(웹 검색, 소셜 네트워크의 콘텐츠 필터링, 전자상거래 웹 사이트의 권장 사항, 카메라와 스마트폰 같은 소비자 제품)에 힘을 실어 준다. Machine Learning 시스템은 이미지에서 물체를 식별하고, 음성을 텍스트로 기록하며, 뉴스 항목, 게시물, 제품을 사용자의 관심사와 일치시키고, 관련 검색 결과를 선택하는 데 사용된다. 점점 더 이러한 응용 프로그램은 Deep Learning이라는 기술 클래스를 사용한다.

기존의 Machine Learning 기법은 자연 데이터를 원시 형태로 처리하는 데 한계가 있었다. 패턴 인식 또는 Machine Learning 시스템을 구축하기 위해서는 학습 하위 시스템이 적절한 내부 표현 또는 특성 벡터로 원시 데이터(예: 이미지의 픽셀 값)를 변환하는 특성 추출기를 설계하기 위해 신중한 엔지니어링과 상당한 영역별 전문 지식이 필요했다. 이러한 분류기는 입력에서 패턴을 탐지하거나 분류할 수 있다. 

Representation Learning은 기계에 원시 데이터를 공급하고 탐지 또는 분류에 필요한 표현을 자동으로 검색할 수 있는 일련의 방법이다. Deep Learning 방법은 다양한 수준의 표현을 가진 Representation Learning 방법으로 각각 하나의 수준(원시 입력부터 시작)에서 표현을 더 높고, 추상적인 수준으로 변환하는 단순하지만 비선형적인 모듈을 구성함으로써 얻어진다. 이러한 변환이 충분히 구성되면 매우 복잡한 함수를 학습할 수 있다. 분류 작업의 경우, 더 높은 표현 계층은 차이의 중요한 입력 측면을 증폭시키고 관련 없는 변동을 억제한다. 예를 들어 이미지는 픽셀 값의 배열 형태로 제공된다. 첫 번째 표현 계층에서 학습된 특징은 일반적으로 이미지의 특정 방향과 위치에서 가장자리의 존재여부를 나타낸다. 두 번째 계층에서는 일반적으로 가장자리 위치의 작은 변동에 관계없이 가장자리의 특정 배열을 감지하여 디자인을 감지한다. 세 번째 계층은 디자인을 익숙한 계체의 일부에 해당하는 더 큰 조합으로 조합할 수 있고, 후속 계층은 이러한 일부의 조합으로 객체를 감지한다. **Deep Learning의 핵심 측면은 이러한 기능 계층이 인간에 의해 설계되지 않고, 범용 학습 절차를 통해 데이터에서 학습한다.**

Deep Learning은 수년 동안 인공지능 커뮤니티의 가장 어려웠던 문제를 해결하는데 큰 역할을 보이고 있다. Deep Learning은 고차원 데이터에서 복잡한 구조를 발견하는 데 매우 능숙한 것으로 밝혀졌으며 이는 많은 과학, 사업, 행정 분야에 적용할 수 있다. 그리고 이미지 인식과 음성 인식에서 기록을 능가하는 것 외에도, 잠재적인 약물 분자의 활동을 예측하고, 입자 가속기 데이터를 분석하고, 뇌 회로를 재구성하고, 비코딩 DNA의 돌연변이가 유전자 발현과 질병에 미치는 영향을 예측하는 다른 Machine Learning 기법들을 능가했다. 더 놀랍게도, Deep Learning은 자연어 이해, 특히 주제 분류, 감정 분석, 질문 답변 및 언어 번역의 다양한 작업에 대해 매우 유망한 결과를 낳았다.

우리는 Deep Learning이 수작업 엔지니어링을 거의 필요로 하지 않기 때문에 사용 가능한 계산과 데이터의 양의 증가를 쉽게 이용할 수 있기 때문에 가까운 미래에 더 많은 성공을 거둘 것이라고 생각한다. 현재 심층 신경망을 위해 개발 중인 새로운 학습 알고리즘과 아키텍처는 이 진전을 가속화한다. 

# Supervised learning
Machibe Learning의 가장 흔한 형태는 Supervised Learning이다. 이미지를 집, 자동차, 사람, 애완동물로 분류할 수 있는 시스템을 만들고 싶다고 상상해 보자. 우리는 먼저 집, 자동차, 사람, 애완동물의 대용량 데이터 세트를 수집한다. 각 이미지에는 해당 범주가 표시되어 있다. 훈련 중에서 기계가 이미지를 보여주고 점수 벡터 형태의 출력을 각 범주마다 하나씩 생성한다. 우리는 원하는 범주가 모든 범주 중에서 가장 높은 점수를 받기를 원하지만 훈련 전에는 이런 일이 일어날 것 같지 않다. 그래서 우리는 출력 점수와 원하는 점수 패턴 사이의 오차(또는 거리)를 측정하는 목표 함수를 계산한다. 그런 다음 기계가 내부 조정 가능한 파라미터를 수정하여 이 오류를 줄입니다. 가중치라고 불리는 이 조절 가능한 파라미터들은 기계의 입출력 기능을 정의하는 "손잡이"로 볼 수 있는 실수이다. 전형적인 Deep Learning 시스템에는 수억개의 이런 조절 가능한 가중치와 기계를 훈련시킬 수 있는 라벨이 붙은 예들이 들어있다. 가중치 벡터를 적절하게 조정하려면, 학습 알고리즘은 각각의 가중치에 대해 가중치가 증가하면서 오차가 얼마나 증가하거나 감소하는지를 나타내는 경사도 벡터를 계산하다. 그런 다음 가중치 벡터가 경사도 벡터와 반대 방향으로 조정된다.

모든 교육 예제의 걸쳐 평균을 낸 목표 기능은 가중치의 가치가 높은 고차원의 공간에서 일종의 경사 지대로 볼 수 있다. 음의 기울기 벡터는 이러한 지형에서 가장 가파른 하강 방향을 나타내기 때문에 낮을수록 출력 오류의 평균이 낮아진다. 

실제로 대부분의 실무자들은 **확률적 경사 하강법(SGD)** 이라는 철차를 사용한다. 이것은 몇 가지 예에 대한 입력 벡터를 보여주고, 출력과 오류를 계산하고, 그러한 예에 대한 평균 기울기를 계산하고, 그에 따라 가중치를 조정하는 것으로 구성된다. 이 프로세스는 목표 기능의 평균 감소가 멈출 때까지 교육 세트의 많은 예에 대해 반복됩니다. 각 예제 집합이 모든 예에 대한 평균 기울기의 노이즈를 추정하기 때문에 확률적이라 불린다. 이 간단한 절차는 일반적으로 훨씬 정교한 최적화 기술과 비교할 때 놀랄 만큼 빠르게 좋은 가중치 집합을 찾는다. 훈련 후, 시스템의 성능은 테스트 세트라고 하는 다른 예제로 측정된다. 이러한 학습 중 본 적이 없는 새로운 입력에 대한 합리적인 답번을 도출할 수 있는 능력은 기계의 일반화 능력을 테스트하는 데 도움이 된다. 

![1](https://github.com/junsu9637/Study/blob/main/Artificial%20Intelligence/Montreal%20AI%20101%20-%20Cheet%20Sheet/Deep%20Learning/Deep%20Learning/Image/1.png?raw=true)

그림 1 : 다층 신경망 및 역전파

> **a.** 다중 계층 신경망(연결된 점으로 표시됨)은 입력 공간을 왜곡하여 데이터 클래스(빨간색과 파란색 라인에 있는 예)를 선형으로 분리할 수 있다. 입력 공간의 일반 격자무늬(왼쪽에 표시됨)도 숨겨진 단위에 의해 변환(중앙 패널에 표시됨)되는 방법을 참고한다. 이 그림은 2개의 입력 장치, 2개의 숨겨진 장치, 1개의 출력 장치로 구성되고, 사물 인식이나 자연어 처리에 사용되는 네트워크는 수만 또는 수십만 개를 포함한다. 이에 대한 자료는 [C.Olah](http://colah.github.io/)의 자료이다.

![6](https://github.com/junsu9637/Study/blob/main/Artificial%20Intelligence/Montreal%20AI%20101%20-%20Cheet%20Sheet/Deep%20Learning/Deep%20Learning/Image/6.png?raw=true)

> **b.** 파생물의 연쇄 법칙은 x와 y, y와 z 간의 작은 변화가 어떻게 나타나는지 알려준다. Δx는 ∂y/∂x(부분 파생물의 정의)를 곱함으로써  Δy로 변환된다. 마찬가지로 Δy의 변화는 Δz를 변화시킨다. 이렇게 Δx에 ∂y/∂x와 ∂z/∂x를 곱하여 Δz로 변하는 방법과 같이 한 방정식을 다른 방정식으로 대체하면 파생물의 연쇄 규칙이 제시된다. 이 이론은 x, y, z가 벡터일 때도 작동한다. 

![7](https://github.com/junsu9637/Study/blob/main/Artificial%20Intelligence/Montreal%20AI%20101%20-%20Cheet%20Sheet/Deep%20Learning/Deep%20Learning/Image/7.png?raw=true)

> **c.** 두 개의 숨겨진 계층과 하나의 출력 계층이 있는 신경망에서 앞으로 값을 전달하는 계산에 사용되는 방정식은 각각 경사도를 역전파할 수 있는 모듈을 구성한다. 각 계층은 각 유닛에 대한 아래 계층의 단위 출력의 가중치 합계인 총 입력 z를 계산하다. 그런 다음 비선형 함수 f()가 z에 적용되어 장치의 출력을 가져온다. 최근 몇 년 동안 다음과 같은 비선형 함수가 신경망에 사용되고 있다. 출력 계층에서, 유닛의 출력과 관련된 오류 파생은 비용 함수를 차별화함으로써 계산된다.
>> **ReLU 함수**          
   *f(z) = max(0,z)*           
   **Sigmoid 함수**         
   *f(z) =1/(1 + exp(−z))*         
   **logistic 함수**        
   *f(z) =(exp(z)− exp(−z))/(exp(z)+exp(−z))*
  
![8](https://github.com/junsu9637/Study/blob/main/Artificial%20Intelligence/Montreal%20AI%20101%20-%20Cheet%20Sheet/Deep%20Learning/Deep%20Learning/Image/8.png?raw=true)
  
> **b.** 뒤로 값을 전달하는 계산에 사용하는 방정식이다. 각 은닉 계층에서 우리는 각 단위의 출력과 관련하여 오류 파생물을 계산한다. 이는 위 계층의 단위에 대한 총 입력과 관련된 오류 파생물의 가중 합계이다. 그런 다음 출력과 관련된 오류 파생물을 입력에 *f(z)* 의 경사도를 곱하여 오류 파생물로 변환한다. 이러게 하면 유닛 *l*에 대한 비용 함수가 *0.5(y<sub>l</sub>-t<sub>l</sub>)<sup>2</sup>* 이면, *y<sub>l</sub>-t<sub>l</sub>* 가 나온다. 여기서 t<sub>l</sub>은 목표값이다. *∂E/∂z<sub>k</sub>* 가 알려지면, 아래 층의 유닛 *j*에서 연결부의 가중치 *w<sub>jk</sub>* 에 대해 파생된 오차는 *y<sub>j</sub>∂E/∂z<sub>k</sub>* 가 된다.


현재 Machine Learning의 많은 실제 응용 프로그램은 수작업으로 설계된 특징 위에 선형 분류기를 사용한다. 2개의 클래스로 구성된 선형 분류기는 특징 벡터 성분의 가중 합계를 계산한다. 가중 합계가 임계값을 초과할 경우, 입력은 특정 범주에 속하는 것으로 분류된다. 

1960년대 이후 우리는 선형 분류기가 입력 공간을 매우 단순한 영역, 즉 초평면(선형 공간의 차원보다 한 단계 낮은 차원을 가진 부분 선형 공간을 평행 이동하여 얻은 평면)으로 분리된 반쪽 공간으로만 분할할 수 있다는 것을 알고 있었다. 그러나 이미지 및 음성 인식과 같은 문제는 입출력 기능이 특정 미세한 변화(예: 차이)에 매우 민감하면서 물체의 위치, 방향 또는 조도의 변화, 음조 또는 말투의 변화 등 입력의 관련되지 않은 변화에는 둔감해야 한다. 픽셀 수준에서 서로 다른 포즈와 다른 환경에서 두 사모예드의 이미지는 서로 매우 다를 수 있다. 반면에 같은 위치와 비슷한 배경에 있는 사모예드와 늑대의 두 이미지는 서로 매우 유사할 수 있다. 선형 분류기 또는 원시 픽셀에서 작동하는 다른 '얕은' 분류기는 앞의 두 분류기를 동일한 범주에 넣는 동안 후자의 두 분류기를 구분할 수 없다. 그렇기 때문에 얕은 분류기에는 선택성-불변성 딜레마를 해결하는 좋은 특징 추출기를 통해 차이에 대한 선별적인 표현을 생성하야 한다. 이러한 표현은 동물의 자세와 같은 무관한 측면에 의해서 변하면 안된다. 분류기를 더 강력하게 만들기 위해 커널 방법과 마찬가지로 일반적인 비선형 기능을 사용할 수 있다. 그러나 가우스 커널에서 발생하는 것과 같은 일반적인 특징들은 학습자가 훈련 예제에서 멀리 떨어진 곳에서 일반화하는 것을 허용하지 않는다. 기존의 방식은 우수한 기능 추출기를 직접 설계하는 것이며, 이는 상당한 양의 엔지니어링 기술과 영역별 전문 지식을 필요로 한다. 그러나 범용 학습 절차를 사용하여 좋은 기능을 자동으로 학습할 수 있다면 이 모든 것을 피할 수 있다. 이것인 Deep Learning의 주요 이점이다. 

Deep Learning 아키텍처는 학습의 대상이 되는 모든(또는 대부분의) 비선형 입력-출력 매핑을 계산하는 단순 모듈의 다중 계층 스택이다. 스택의 각 모듈은 입력 정보를 변환하여 표현의 선택성과 불변성을 모두 증가시킨다. 깊이가 5~20인 여러 개의 비선형 계층을 사용할 경우, 시스템은 동시에 민감한 입력의 매우 복잡한 기능을 구현할 수 있다. 예를 들어 사모예드와 흰 늑대를 구별하는 것에서 배경, 포즈, 조명, 주변 객체와 같은 관계 없는 큰 변형에는 무감각해 진다.

![2](https://github.com/junsu9637/Study/blob/main/Artificial%20Intelligence/Montreal%20AI%20101%20-%20Cheet%20Sheet/Deep%20Learning/Deep%20Learning/Image/2.png?raw=true)

그림 2 : 컨볼루션 네트워크 내부

> 왼쪽 아래 사모예드 이미지에 적용되는 일반적인 컨볼루션 네트워크 아키텍쳐의 각 계층의 출력이다. 각 직사각형 이미지는 학습된 기능 중 하나에 대한 출력에 해당하는 기능 맵입니다. 정보는 아래에서 위로 흐르며, 하위 레벨 기능은 방향 가장자리 감지기로 작동하고, 점수는 출력의 각 이미지 클래스에 의해 계산된다.


# 여러 계층 아키텍처 교육을 위한 Backpropagation

패턴 인식 초기부터 연구원들의 목적은 손으로 만든 기능을 훈련 가능한 다층 네트워크로 대체하는 것이었지만, 이 문제는 1980년대 중반까지 널리 이해되지 못했다. 위에서 이야기한 바와 같이 다층 아키텍처는 단순한 확률적 경사 하강에 의해 훈련될 수 있다. 모듈이 입력과 내부 가중치에 비교적 부드러운 기능을 제공한다면 역전파 절차를 이용하여 경사도를 계산할 수 있다. 이런 생각은 1970년대와 1980년대에 몇몇 다른 그룹들에 의해 독립적으로 발견되었다.

모듈 다층 스택의 가중치와 관련하여 목적 함수의 기울기를 계산하는 역전파 절차는 파생 모델에 대한 체인 규칙의 실용적인 적용에 지나지 않는다. 이것의 핵심은 모듈의 입력(또는 후속 모듈의 입력)과 관련하여 기울기에서 역방향으로 작업함으로써 모듈의 파생물(또는 기울기)을 계산할 수 있다는 것이다(그림 1 참조). 역전파 방정식은 모든 모듈을 통해 (네트워크가 예측을 생성하는) 상단의 출력에서 시작하여 (외부 입력이 공급되는) 하단으로 이동하면서 경사도를 전파하는 데 반복적으로 적용될 수 있다. 이러한 경사도를 계산한 후에는 각 모듈의 가중치에 대한 경사도를 계산하는 것이 간단해진다.

Deep Learning의 많은 응용 프로그램은 feed-forward 신경망 아키텍처(실행 전에 결함을 예측하고 행하는 피드백 과정의 제어)를 사용하여(그림 1 참조) 고정 크기 입력(예: 이미지)을 고정 크기 출력(예: 여러 범주에 대한 확률)에 매핑하는 방법을 배운다. 한 계층에서 다음 계층으로 이동하기 위해, 단위 집합은 이전 계층으로부터 입력의 가중 합계를 계산하고 비선형 함수를 통해 결과를 전달한다. 현재 가장 많이 사용되는 비선형 함수는 반파장 정류기인 ReLU(정류 선형 유닛)이다. 지난 수십 년 동안 신경망은 부드러운 비 선형성을 사용하는 tanh(z)나 1/(1+exp(−z))를 사용했다. 하지만 ReLU는 일반적으로 많은 계층이 있는 네트워크에서 훨씬 더 빨리 학습하여 비지도 사전 교육 없이 심층적인 감독 네트워크를 훈련할 수 있다. 입력 또는 출력 계층에 없는 단위를 은닉 계층아리고 한다. 은닉 계층은 마지막 계층에 의해 범주가 선형적으로 분리될 수 있도록 비선형 방식으로 입력을 왜곡하는 것으로 볼 수 있다(그림 1 참조).

1990년대 후반, 신경망과 역전파는 주로 Machine Learning 커뮤니티에 의해 버려졌고 컴퓨터 비전 및 음성 인식 커뮤니티에 의해 무시되었다. 사전 지식이 거의 없는 유용한 다단계 특징 추출기를 학습하는 것은 불가능하다고 널리 생각되었다. 특히, 일반적으로 단순한 경사 하강이 작은 변화 없이 평균 오차를 줄일 수 있는 가중치로 구성된 지역 최소치에 갇힐 것으로 생각되었다.

실제로, 열악한 지역 최소화는 대형 네트워크에서 거의 문제가 되지 않는다. 이러한 시스템은 초기 조건에 상관없이 시스템은 거의 항상 매우 유사한 품질의 해결책에 도달한다. 최근의 이론, 경험적 결과는 국소적 최소화가 일반적으로 심각한 문제가 아님을 강력히 시사한다. 대신 경사가 0인 안장점이 조합적으로 많은 지형이 채워지고, 표면은 대부분의 차원으로 곡선을 이룬다. 이러한 내용을 분석한 결과, 몇 개의 하향 곡면 방향만 있는 안장점이 매우 많이 존재하는 것으로 나타났지만 거의 모든 것들이 목적 함수의 유사한 가치를 가지고 있다. 따라서 알고리즘이 어느 안장 지점에 고착되는지는 그다지 중요하지 않다.

심층 feed-forward 네트워크에 대한 관심은 2006년 캐나다 고등 연구소(CIFAR)에 의해 되살아났다. 연구진은 라벨링된 데이터를 요구하지 않고 기능 검출기의 계층을 생성할 수 있는 비지도 학습 절차를 도입했다. 형상 검출기의 각 계층을 학습하는 목적은 아래 계층에서 형상 검출기(또는 원시 입력)의 활동을 재구성하거나 모델링하는 것이다. 이 재구성 목표를 사용하여 점진적으로 더 복잡한 형상 검출기의 여러 계층을 '사전 훈련'함으로써 심층 네트워크의 가중치는 합리적인 값으로 초기화될 수 있다. 그런 다음 출력 장치의 최종 계층 네트워크 상단에 추가되고 전체 심층 시스템이 표준 역전파를 사용하여 미세 조정될 수 있다. 이는 특히 손으로 쓴 숫자를 인식하거나 보행자를 감지하는 것과 같이 라벨링된 데이터의 양이 매우 제한적일 때 매우 효과적이었다.

이러한 사전 훈련 접근법은 음성 인식에 처음으로 적용되었다.  이는 프로그래밍이 편리하고 연구자들이 네트워크를 10배 또는 20배 더 빠르게 훈련할 수 있는 빠른 그래픽 처리 장치(GPU)의 출현으로 가능해졌다. 이 접근 방식은 음파에서 추출한 계수의 짧은 시간 창을 창 중앙의 프레임으로 나타낼 수 있는 다양한 음성 조각에 대한 확률 집합에 매팡하는데 사용되었다. 이것은 작은 어휘를 사용한 표준 음성 인식 벤치마크에서 기록적인 결과를 달성했고, 큰 어휘 작업에 대한 기록적 결과를 제공하기 위해 빠르게 개발되었다. 2009년의 딥넷 버전은 많은 주요 음성 그룹에 의해 개발되고 있었고 2012년 안드로이드 폰에 이미 배치되어 있었다. 소규모 데이터 세트에서 감독되지 않은 사전 훈련은 과적합을 방지하는 데 도움이 되며, 라벨링된 예제의 수가 적을 때 또는 일부 '소스' 작업에 대한 예는 많지만 일부 '대상' 작업에 대한 예는 거의 없는 전송 설정에서 훨씬 더 나은 일반화로 이어진다. Deep Learning이 재활성화된 후, 사전 교육 단계는 작은 데이터 세트에만 필요한 것으로 밝혀졌다.

그러나, 인접한 계층들 사이의 완전한 연결을 가진 네트워크보다 훨씬 더 훈련하기 쉽고 일반화된 특정한 유형의 심층 feed-forward 네트워크가 있었다. 이것은 Convolutional Neural Network(ConvNet)이다. 이것은 신경망의 인기가 떨어졌던 기간 동안 많은 실질적인 성공을 거두었고 최근 컴퓨터 비전 커뮤니티에 의해 널리 채택되었다.

# Convolutional neural networks

ConvNets은 여러 배열의 형태로 제공되는 데이터(예: 3가지 컬러 채널에서 픽셀 강도를 포함하는 3개의 2D 어레이로 구성된 컬러 이미지)를 처리하도록 설계되었다.

> 1D : 언어를 포함한 신호 및 순서         
  2D : 영상, 오디오 스펙트럼 프로그램         
  3D : 비디오, 볼륨 이미지
  
ConvNets 뒤에는 자연 신호의 속성을 활용하는 4가지 핵심 아이디어(지역 연결, 공유 가중치, 풀링, 많은 계층 사용)가 있다. 

일반적인 ConvNet의 아키텍처(그림 2 참조)는 일련의 단계로 구성된다. 처음 몇 단계는 두 가지 유형(컨볼루션 계층, 풀링 계층)의 레이어로 구성된다. 컨볼루션 계층의 단위는 특징 맵에서 구성되며, 각 단위는 필터 뱅크라는 가중치 집합을 통해 이전 계층의 특징 맵에서 지역 조각에 연결된다. 그런 다음 이 국소 가중 합계의 결과는 ReLU와 같은 비선형성을 통과한다. 모든 유닛은 특징 맵의 모든 단위가 동일한 필터 뱅크를 공유하고, 한 계층의 서로 다른 특징 맵은 서로 다른 필터 뱅크를 사용한다. 이러한 아키텍쳐를 사용하는 이유는 아래와 같다.

> 1. 이미지와 같은 배열 데이터에서 값의 지역 그룹이 높은 상관관계를 갖고, 쉽게 감지되는 고유한 지역 디자인을 형성한다.         
  2. 영상 및 기타 신호의 지역 통계는 불변한다.       

디자인이 이미지의 한 부분에 나타날 수 있다면 서로 다른 위치에 있는 장치들이 동일한 가중치를 공유하고 배열의 다른 부분에서 동일한 패턴을 감지할 수 있다. 그래서 형상도에 의해 수행되는 필터링 작업은 이산 컨볼루션 이라는 수학적인 이름이 붙었다. 

비록 컨볼루션 계층의 역할은 이전 레이어에서 형상의 로컬 연결을 감지하는 것이지만 풀링 계층의 역할은 의미론적으로 유사한 기능을 하나로 병합하는 것이다. 디자인을 형성하는 형상의 상대적 위치가 다소 다를 수 있기 때문에 각 형상의 위치를 거칠게 그려 디자인을 신뢰성 있게 검출할 수 있어야 한다. 일반적인 풀링 장치는 하나의 특징 맵(또는 몇 개의 특징 맵)에서 단위의 지역 조각의 최대값을 계산한다. 인접 풀링 유닛은 둘 이상의 행 또는 열에 의해 이동되는 조각으로부터 입력을 받아 표현 차원을 줄이고 작은 이동과 왜곡에 대한 불편성을 생성한다. 풀링으로 만들어진 비선형성 2,3단계 컨볼루션은 더 많은 컨볼루션 및 완전히 연결된 계층이 뒤따른다. ConvNet을 통해 경사도를 역전파하는 것은 일반 심층 네트워크를 통과하는 것만큼 간단하며, 모든 필터 뱅크의 모든 가중치를 훈련시킬 수 있다.

심층 신경망은 많은 자연 신호가 구성 계층이라는 특성을 활용하는데, 여기서 하위 수준을 구성함으로써 더 높은 수준의 특징을 얻는다. 이미지에서 가장자리의 국부적인 조합은 디자인을 형성하고, 디자인은 조각을 조립하고, 조각은 객체를 형성한다. 음성에서 전화, 음소, 음절, 단어 및 문장에 이르는 언어와 텍스트에도 유사한 계층이 존재한다. 풀링을 사용하면 이전 계층의 요소가 위치와 모양이 다를 때 표현이 거의 변경되지 않는다. 

ConvNets의 컨볼루션 및 풀링 계층은 시각적 신경 과학의 단순한 세포와 복잡한 세포의 고전적인 개념에서 영감을 받았다. 그리고 전체 아키텍쳐는 시각 피질 복부 경로 내부의 LGN–V1–V2–V4–IT 계층을 연상시킨다. ConvNet 모델과 원숭이에게 같은 그림을 보여줄 때 ConvNet에서 높은 레벨의 유닛의 활성화는 원숭이의 IT에서 160개의 뉴런들 중 무작위한 절반의 변화를 설명한다. ConvNets은 신인식기(신경 회로망에서의 패턴 인식 모델)에 뿌리를 두고 있다. 그러나 역전파와 같은 end-to-end 감독 학습 알고리즘은 가지고 있지 않다. 시간 지연 신경망이라 불리는 원시 1D ConvNet은 음소 및 간단한 단어의 인식에 사용되었다.

1990년대 초반에 수많은 컨볼루션 네트워크의 응용이 있었다. 이는 음성 인식 및 문서 읽기를 위한 시간 경과에 따른 신경망으로 시작한다. 문서 읽기 시스템은 언어 제약을 구현한 확률적 모델과 공동으로 훈련된 ConvNet을 사용했다. 많은 ConvNet 기반의 광학 문자 인식 및 필기 인식 시스템은 나중에 마이크로소프트에 의해 구현되었다. ConvNets는 또한 얼굴 인식을 위해 얼굴과 손을 포함한 자연 이미지에서 물체 감지가 연구되었다.

# Deep Convolutional neural networks를 통한 이미지 이해

2000년대 초반부터 ConvNets은 영상에서 객체 및 영역의 분할 및 인식에서 큰 성공을 거두었다. 이는 교통 표지 인식을 위한 생물학적 이미지의 분할과 자연 이미지에서 얼굴, 텍스트, 보행자 및 인체의 검출 등 라벨링 데이터가 상대적으로 풍부한 모든 작업이었다. ConvNets의 최근 실질적인 주요 성공은 얼굴 인식이다.

화소 수준에서 이미지를 라벨링할 수 있다는 것은 자율주행 로봇 및 자율주행차를 포함한 기술 분야에 응용 될 것이다. Mobileye와 NVIDIA와 같은 회사들은 곧 출시될 자동차 비전 시스템에 ConvNet 기반 방법을 사용하고 있다. 

그러나 ConvNet은 2012년까지는 ImageNet 경쟁에서 주류 컴퓨터 비전 및 기계 학습 커뮤니티에 의해 배척당했었다. 심층 컨볼루션 네트워크는 1,000개의 다른 클래스를 포함하는 웹의 약 백만 개의 이미지 데이터 세트를 적용하여 최고의 경쟁 접근 방식의 오류율을 거의 절반으로 줄이는 놀라운 결과를 얻었다. 이러한 성공은 GPU, ReLUs, 드롭아웃이라는 새로운 정규화 기법 및 기존 기술을 변형하여 더 많은 교육 사례를 생성하는 기법에서 비롯되었다. 이 성공은 컴퓨터 비전에 혁명을 가져왔다. ConvNets는 이제 거의 모든 인식 및 감지 작업에 대한 지배적인 접근 방식이며 일부 작업에서 인간 성능에 접근한다. 그림 3은 이미지 캡션 생성을 위한 ConvNets과 반복 넷 모듈을 결합한 최근 대모 방식이다.

![3](https://github.com/junsu9637/Study/blob/main/Artificial%20Intelligence/Montreal%20AI%20101%20-%20Cheet%20Sheet/Deep%20Learning/Deep%20Learning/Image/3.png?raw=true)

그림 3 : 이미지에서 텍스트로

> 반복 신경망(RNN)에 의해 생성된 캡션은 테스트 이미지에서 심층 컨볼루션 신경망(CNN)에 의해 추출된 표현을 테스트 이미지에서 캡션(위)으로 '번역'하도록 훈련된 것이다. RNN이 각 단어(볼드)를 생성할 때 입력 이미지의 다른 위치(중반부와 하단)에 주의를 집중할 수 있는 기능이 주어졌을 때, 우리는 RNN이 이미지를 캡션으로 더 나은 '번역'을 달성하기 위해 이를 활용하는 것을 발견했다.

최신 ConvNet 아키텍처에는 ReLU 10~20개의 계층, 수억 개의 가중치, 장치 간 수십억 개의 연결이 있다. 몇년 전의 하드웨어의 발전은 대규모 네트워크를 훈련시키는 데는 몇 주가 걸릴 수 있었지만, 소프트웨어와 알고리즘 병렬화는 훈련 시간을 몇 시간으로 줄였다. ConvNet 기반의 비전 시스템의 성능은 Google, Facebook, Microsoft, IBM, Yahoo!, Twitter, Adobe를 포함한 대부분의 주요 기술 회사들이 연구 개발 프로젝트를 시작하고, 이미지 이해의 제품 및 서비스를 구축했다. 

ConvNets는 칩 또는 현장에서 프로그래밍할 수 있는 게이트 배열의 효율적인 하드웨어 구현에 쉽게 적응할 수 있다. NVIDIA, Mobileye, Intel, Qualcomm, Samsung 등 다수의 기업이 스마트폰, 카메라, 로봇, 자율주행차 등에서 실시간 비전 애플리케이션을 지원하기 위해 ConvNet 칩을 개발하고 있다.

# 분산 표현 및 언어 처리

Deep Learning 이론은 Deep Net이 분산 표현을 사용하지 않는 고전적인 학습 알고리즘에 비해 두 가지 지수적 이점을 가지고 있다. 이러한 두 가지 이점은 모두 구성의 힘에서 비롯되며 적절한 구성 요소 구조를 가진 기본 데이터 생성 분포에 의존한다. 

```markdown
1. 분산 표현을 학습하려면 교육 중에 볼 수 있는 값을 넘어 학습된 기능의 값의 새로운 조합으로 일반화할 수 있다(예 : 2n조합은 n개의 이진 기능으로 표헌 가능하다).
2. 심층 네트워크에서 표현 계층을 구성하면 또 다른 지수적 이점이 발생할 가능성이 있다.
```

다중 계층 신경망의 숨겨진 계층은 목표 출력을 쉽게 예측할 수 있는 방식으로 네트워크의 입력을 나타내는 방법을 배운다. 이는 다층 신경망을 훈련시켜 이전 단어의 지역 맥락에서 다음 단어를 순차적으로 예측하는 것으로 잘 입증되었다. 맥락의 각 단어는 N의 단일 벡터로서 네트워크에 1 또는 0으로 제시된다. 첫 번째 계층에서 각 단어는 활성화 패턴이나 단어 벡터를 활용하여 다른 단어를 만든다(그림 4 참조).

![4](https://github.com/junsu9637/Study/blob/main/Artificial%20Intelligence/Montreal%20AI%20101%20-%20Cheet%20Sheet/Deep%20Learning/Deep%20Learning/Image/4.png?raw=true)

그림 4 : 학습된 단어 벡터 시각화

> 외쪽에는 t-SNE 알고리즘을 이용한 시각화를 위해 2D의 비선형 투영과 같은 모델링 언어를 위해 학습된 단어 표현의 그림이 있다. 오른쪽은 영어-프랑스어 인코더-디코더 반복 신경망에서 학습한 구문의 2D 표현이다. 의미론적으로 유사한 단어 또는 단어의 순서가 근처의 표현에 매핑된다는 것을 관찰할 수 있다. 단어의 분산 표현은 역전파를 사용하여 각 단어에 대한 표현과 사건의 다음 단어(언어 모델링용) 또는 전체 번역 단어(기계 번역용)와 같은 목표량을 예측하는 함수를 공동으로 학습함으로써 얻어진다.

언어 모델에서, 네트워크의 다른 계층들은 입력 워드 벡터를 예측된 다음 단어에 대한 출력 워드 벡터로 변환하는 것을 배운다. 이는 어휘의 어떤 단어가 다음 단어로 나타날 확률을 예측하는 데 사용될 수 있다. 네트워크는 기호에 대한 분산 표현을 학습하는 맥락에서 처음 나타났듯이 단어의 개별 특징으로 해석될 수 있는 많은 활성 구성요소를 포함하는 단어 벡터를 학습한다. 이러한 의미적 특성은 입력에 명시적으로 존재하지 않았다. 이들은 입력과 출력 기호 사이의 구조적 관계를 여러 '마이크로 규칙'으로 인수하는 좋은 방법으로 학습 절차에 의해 발견되었다. 단어 순서가 실제 텍스트의 큰 언어 집합에서 나오고 개별 마이크로 규칙을 신뢰할 수 없을 때 단어 벡터를 학습하는 것도 매우 잘 작동하는 것으로 나타났다. 예를 들어 뉴스 기사에서 다음 단어를 예측하도록 훈련받았다면 화요일과 수요일의 학습된 단어 벡터는 비슷할 것이다. 요소(특징)가 상호 배타적이지 않고 많은 구성이 관측 데이터에서 보이는 변동에 해당하기 때문에 이러한 표현을 분산 표현이라고 한다. 이 단어 벡터는 전문가가 미리 결정한 것이 아니라 신경망에 의해 자동으로 발견된 학습된 특징으로 구성된다. 텍스트에서 배운 단어의 벡터 표현은 현재 자연어 응용 분야에서 매우 널리 사용되고 있다.

표현 문제의 핵심은 논리에서 영감을 받은 것과 신경망에서 영감을 받은 인지 패러다임 사이에 존재한다. 논리적으로 영감을 받은 패러다임에서 상징의 사례는 다른 기호 사례와 동일하거나 동일하지 않은 유일한 속성이다. 이것은 사용과 관련된 내부 구조가 없다. 그리고 기호로 추론하면 현명하게 선택된 추론 규칙들의 변수들에 속박되어야 한다. 반대로 신경망은 쉬운 상식 추론을 뒷받침하는 빠른 '직관적'추론 유형을 수행하기 위해 오직 큰 가중치 행렬과 스칼라 비선형성 벡터만 사용한다. 

신경 언어 모델을 도입하기 전에는 언어의 통계적 모델링에 대한 표준 접근 방식은 분산 표현을 이용하지 않았다. 이는 N-grams라고 불리는 N개의 기호 사건의 발생 빈도를 계산하는데 기초하였다. 가능한 N-gram의 수는 VN의 순서로 되어 있다(V : 어휘 크기). 따라서 소수의 단어 이상의 문맥을 고려하면 매우 큰 교육 언어 집합이 필요하다. N-gram은 각각의 단어를 원자 단위로 취급한다. 그래서 의미론적으로 관련된 단어의 순서를 일반화 할 수 없다. 신경 언어 모델은 각 단어를 실제 가치 있는 특징의 벡터와 연관시키기 때문에 일반화가 가능하다. 아는 의미론적으로 관련된 단어들은 그 벡터 공간에서 서로 밀접하게 연결된다

# 반복 신경망

역전파가 처음 도입되었을 때, 그것의 가장 흥미로운 용도는 반복 신경망(RNN)을 훈련시키는 것이었다. 언어나 언어 같은 순차적 입력을 포함하는 작업의 경우, RNN을 사용하는 것이 더 좋은 경우가 많다(그림 5 참조). RNN은 한 번에 한 요소씩 입력 배열을 처리한다. 숨겨진 단위에서 배열의 모든 과거 요소에 대한 정보를 암시적으로 포함하는 '상태 벡터'를 유지한다. 서로 다른 이산 시간 단계에서 숨겨진 단위의 출력을 깊은 다층 네트워크에서 서로 다른 뉴런의 출력인 것처럼 고려할 때(그림 5, 오른쪽), RNN 교육에 역전파를 적용하는 방법이 명확해진다.

![5](https://github.com/junsu9637/Study/blob/main/Artificial%20Intelligence/Montreal%20AI%20101%20-%20Cheet%20Sheet/Deep%20Learning/Deep%20Learning/Image/5.png?raw=true)

그림 5 : 반복 신경망과 전방 계산에 관련된 계산 시간의 전개

> 인공 뉴런(예: 시간 t에 값이 있는 노드 아래에 그룹화된 숨겨진 단위)은 이전 시간 단계에서 다른 뉴런으로부터 입력을 받는다(이것은 검은색 정사각형으로 표현되며, 왼쪽의 한 시간 스텝의 지연을 나타냄). 이러한 방식으로 반복 신경망은 모든 이전 x<sup>'</sup><sub>t</sub>(t<sup>'</sup> ≤ t)에 따라 x<sub>t</sub>요소를 포함한 입력 시퀀스를 각 o<sub>t</sub>와 함께 출력 배열로 매핑할 수 있다. 각 시간 단계에서 동일한 한도(Matrix U,V,W)가 사용된다. 네트워크가 출력 배열를 생성할 수 있는 변형(예: 단어)을 포함한 많은 다른 아키텍처가 가능하며, 각 아키텍처가 다음 시간 단계의 입력으로 사용된다. 역전파 알고리즘(그림 1 참조)은 오른쪽의 펼쳐진 네트워크의 계산 그래프에 직접 적용하여 모든 상태 s<sub>t</sub> 및 모든 매개 변수에 대한 총 오류(예: 올바른 출력 시퀀스를 생성하는 로그 확률)의 파생물을 계산할 수 있다.

RNN은 매우 강력한 동적 시스템이지만, 역전파 경사도가 각 시간 단계에서 증가하거나 감소하여 많은 시간 단계에서 일반적을 폭발하거나 사라지기 때문에 이를 훈련하는 것은 문제가 있는 것으로 판명되었다.

아키텍처의 진보와 이들을 훈련시키는 방법 덕분에 RNN은 텍스트의 다음 문자나 배열의 다음 단어를 매우 잘 예측하는 것으로 밝혀졌고, 더 복잡한 작업에 사용될 수도 있다. 예를 들어 영어 문장을 한 번에 한 단어씩 읽고 나면, 영어 '인코더' 네트워크는 숨겨진 단위의 최종 상태 벡터가 문장에 의해 표현된 생각을 잘 표현하도록 훈련될 수 있다. 그런 다음 이 사고 벡터는 공동으로 훈련된 프랑스 '디코더' 네트워크의 초기 숨겨진 상태로 사용하여 프랑스어 번역의 첫 번째 단어에 대한 확률 분포를 출력할 수 있다. 만약 이 배포에서 특정 첫 번째 워드가 선택되고 디코더 네트워크에 입력으로 제공된다면, 변환의 두 번째 워드에 대한 확률 분포를 출력하여 완전 정지가 선택될 때까지 출력한다. 전반적으로, 이 과정은 영어 문장에 의존하는 확률 분포에 따라 프랑스어 단어의 배열을 생성한다.
기계번역을 하는 다소 순진한 방법은 빠르게 최첨단 기술과 경쟁하게 되었다. 그리고 이것은 문장을 이해하는 것이 추론 규칙을 사용하여 조작되는 내부 상징적 표현과 같은 것을 요구하는지에 대해 심각한 의문을 제기한다. 이것은 일상적 추론이 결론에 대한 신뢰성에 기여하는 많은 동시적 유사성을 수반한다는 견해와 더 호환된다.

프랑스 문장의 의미를 영어 문장으로 번역하는 대신 이미지의 의미를 영어 문장으로 '번역'하는 법을 배울 수 있다(그림 3 참조). 여기서 인코더는 픽셀을 마지막 숨겨진 계층에서 활동 벡터로 변환하는 심층 ConvNet이다. 디코더는 기계 번역 및 신경 언어 모델링에 사용되는 것과 유사한 RNN이다. 최근 이러한 시스템에 대한 관심이 급증하고 있다. 

RNN이 제대로 펼쳐지면(그림 5 참조), 모든 계층이 동일한 가중치를 공유하는 매우 심층적인 feed-forward 네트워크로 볼 수 있다. 이들의 주된 목적은 장기적인 의존성을 배우는 것이지만, 이론적이고 경험적인 증거는 매우 오랫동안 정보를 저장하는 것을 배우는 것이 어렵다는 것을 보여준다.

이를 수정하는 방법은 하나의 아이디어는 명시적 메모리로 네트워크를 증강하는 것이다. 이러한 종류의 첫 번째 제안은 입력을 오랫동안 기억하고 특수 숨겨진 장치를 사용하는 LSTM 네트워크이다. 여기에서 기억 세포라고 불리는 특별한 단위는 계산기나 게이트 누수 뉴런과 같은 역할을 한다. 이것은 1개의 가중치가 다음 단계와 자신이 연결되어 있다. 따라서 자체적인 실제 값 상태를 복사하고 외부 신호를 축적하지만, 이 자체 연결은 메모리의 내용을 지우는 시기를 결정하는 것을 배우는 다른 장치에 의해 곱셈으로 게이트된다. 

LSTM 네트워크는 각 시간 단계에 대해 여러 개의 계층이 있을 때 기존의 RNN보다 더 효과적인 것으로 입증되어 음향에서 문자 배열에 이르는 전체 음석 인식 시스템을 가능하게 만들었다. LSTM 네트워크 또는 관련 형태의 게이트 장치는 기계 번역에서 매우 우수한 성능을 발휘하는 인코더 및 디코더 네트워크에도 현재 사용되고 있다.

지난 1년 동안 여러 저자들은 메모리 모듈로 RNN을 증강하는 다른 제안을 해왔다. 이러한 제안에는 RNN이 읽거나 쓰기로 선택할 수 있는 '테이프 같은' 메모리에 의해 네트워크가 증강되는 신경 튜링 기계가 포함된다. 그리고 메모리 네트워크는 일반적인 네트워크가 연상기억의 일종에 의해 증강된다. 메모리 네트워크는 표준 질문 답변 벤치마크에서 우수한 성능을 보였다. 메모리는 네트워크가 나중에 질문에 답하도록 요청받은 이야기를 기억하기 위해 사용된다. 

신경 튜링 기계와 메모리 네트워크는 일반적으로 단순한 암기를 넘어 추론과 기호 조작이 필요한 작업에 사용되고 있다. 신경 튜링 기계는 '알고리즘'을 가르칠 수 있다. 무엇보다도, 그들은 입력이 정렬되지 않은 순서로 구성되었을 때 정렬된 기호 목록을 출력하는 것을 배울 수 있다. 이 순서는 각 기호가 목록의 우선순위를 나타내는 실제 값에 수반된다. 메모리 네트워크는 텍스트 어드벤처 게임과 유사한 설정에서 세계의 상태를 추적하도록 훈련될 수 있으며, 이야기를 읽은 후에는 복잡한 추론이 필요한 질문에 대답할 수 있다. 한 테스트 예에서, 네트워크는 "반지의 제왕"의 15문장으로 보여지고 "프로도는 지금 어디에 있나요?"와 같은 질문에 정확하게 답한다. 

# Deep Learning의 미래

Unsupervised Learning은 Deep Learning에 대한 관심을 되살리는 촉매 작용을 했지만, 그 후 순수하게 지도된 학문의 성공에 가려져 왔다. 본 문서에서 우리는 그것에 초점을 맞추지 않았지만, 우리는 Unsupervised Learning이 장기적으로 훨씬 더 중요해질 것으로 예상한다. 인간과 동물의 학습은 대부분 감독되지 않는다. 우리는 세상의 구조를 모든 물체의 이름을 듣는 것이 아니라 그것을 관찰하면서 발견한다. 

인간 비전은 큰 저해상도 서라운드를 가진 작은 고해상도 포바를 사용하여 지능적이고 작업별 방식으로 광학 배열을 순차적으로 샘플링하는 활성 프로세스이다.
우리는 향후 비전 진행의 상당 부분이 end-to-end로 훈련되고 강화 학습을 사용하여 어디를 찾아야 할지 결정하는 RNN과 ConvNets를 결합한 시스템에서 나올 것으로 예상한다. Deep Learning과 Reinforcement Learning을 결합한 시스템은 초기 단계에 있지만 분류 작업에서 이미 패시브 비전 시스템을 능가하고 다양한 비디오 게임을 학습하는 데 인상적인 결과를 만들어 낸다.

자연어 이해는 향후 몇 년 동안 Deep Learning이 큰 영향을 미칠 또 다른 분야이다. 우리는 RNN을 사용하여 문장이나 전체 문서를 이해하는 시스템이 한 번에 한 부분을 선택적으로 참석하기 위한 전략을 학습할 때 훨씬 더 나아질 것으로 기대한다. 

궁극적으로, 인공지능의 주요 발전은 표현 학습과 복잡한 추론을 결합한 시스템을 통해 이루어질 것이다. 오랫동안 음성 및 필기 인식에 Deep Learning과 간단한 추론이 사용되었지만, 큰 벡터에 대한 연산에 의한 기호 표현의 규칙 기반 조작을 대체하기 위한 새로운 패러다임이 필요하다.
